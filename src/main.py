import argparse
import os
import sys
from langdetect import detect
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import pdfplumber

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏ –º–æ–¥–µ–ª–∏
SUPPORTED_LANGS = {'en', 'ru', 'de'}

# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —è–∑—ã–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
LANG_MAP = {
    'en': 'english',
    'ru': 'russian',
    'de': 'german'
}

def load_text(file_path: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ .txt –∏–ª–∏ .pdf —Ñ–∞–π–ª–∞"""
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
    
    if file_path.lower().endswith('.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    elif file_path.lower().endswith('.pdf'):
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ .txt –∏ .pdf —Ñ–∞–π–ª—ã")

def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É"""
    try:
        lang = detect(text)
    except:
        lang = 'en'  # fallback
    
    if lang not in SUPPORTED_LANGS:
        print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫: {lang}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ fallback.")
        lang = 'en'
    return lang

def summarize_text(text: str, lang: str, compression: int, max_input_length=1024):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é mT5"""
    if not text.strip():
        return "–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç"
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–º–æ–∂–Ω–æ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –≤ –±—É–¥—É—â–µ–º)
    model_name = "csebuetnlp/mT5_multilingual_xlsum"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –º–æ–¥–µ–ª–∏
    inputs = tokenizer(
        text,
        return_tensors="pt",
        max_length=max_input_length,
        truncation=True,
        padding="max_length"
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∂–∞—Ç–∏—è
    input_len = len(tokenizer.tokenize(text))
    target_len = max(30, int(input_len * compression / 100))
    max_len = min(512, target_len)
    min_len = max(20, int(max_len * 0.7))

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_length=max_len,
            min_length=min_len,
            length_penalty=1.0,
            num_beams=4,
            early_stopping=True
        )

    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

def main():
    parser = argparse.ArgumentParser(description="Multilingual Learning Material Summarizer")
    parser.add_argument("--input", "-i", required=True, help="–ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É (.txt –∏–ª–∏ .pdf)")
    parser.add_argument("--language", "-l", choices=["auto", "en", "ru", "de"], default="auto",
                        help="–Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)")
    parser.add_argument("--compression", "-c", type=int, choices=[20, 30, 50], default=30,
                        help="–£—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (20, 30, 50)")
    parser.add_argument("--output", "-o", help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª")

    args = parser.parse_args()

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
    try:
        text = load_text(args.input)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        sys.exit(1)

    if not text.strip():
        print("‚ùå –§–∞–π–ª –ø—É—Å—Ç–æ–π")
        sys.exit(1)

    # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
    if args.language == "auto":
        lang = detect_language(text)
    else:
        lang = args.language

    if lang not in SUPPORTED_LANGS:
        print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫: {lang}")
        sys.exit(1)

    print(f"üî§ –û–±–Ω–∞—Ä—É–∂–µ–Ω —è–∑—ã–∫: {LANG_MAP[lang]} ({lang})")
    print(f"üìâ –£—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è: {args.compression}%")

    # 3. –†–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ... (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1‚Äì2 –º–∏–Ω—É—Ç—ã –Ω–∞ CPU)")
    summary = summarize_text(text, lang, args.compression)

    # 4. –í—ã–≤–æ–¥
    print("\n" + "="*60)
    print("üìù –†–ï–ó–Æ–ú–ï:")
    print("="*60)
    print(summary)

    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(summary)
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {args.output}")

if __name__ == "__main__":
    main()